[
["index.html", "텍스트 분석을 통한 프로듀스X101 데뷔조 예측 Chapter 1 프롤로그", " 텍스트 분석을 통한 프로듀스X101 데뷔조 예측 JDW 2019-12-19 Chapter 1 프롤로그 조금은 시간이 지났지만 지난 여름 프로듀스X 101이 한참 방영했던 당시 진행했던 텍스트 분석입니다. k-pop에 관심이 있으신 분이라면 아실만한 프로듀스 시리즈는 시즌을 거듭할수록 화제가 높아져만 갔는데요. 그 중 이번에 분석한 프로듀스 X 101은 엠넷의 프로듀스 시리즈 중 네번째 시리즈로 101명의 연습생들중 국민프로듀서(시청자) 들이 투표를 통해 열 한명의 아이돌을 선발한다는 컨셉의 방송입니다. 사실 지금 이 글을 쓰고있는 시점인 11월 현재 프로듀스 시리즈는 제작진의 투표조작 논란에 휩싸여서 국민 프로듀서의 투표를 통해 아이돌을 뽑는다는 컨셉의 신뢰성이 무너진 상태라 텍스트 분석을 통한 데뷔조 예측이 의미가 있긴 했었을까라는 의문을 가질수도 있는데요. 그래도 분석을 통해 어느 연습생의 인기가 많았는지 또는 누가 많은 이슈를 몰고 있었는지를 가늠할 수 있었던 척도로 바라볼 수 있어서 나름 의미있는 분석이였다고 생각이 듭니다. 데이터 분석은 크게 세가지로 나누어 진행을 하였습니다. 데이터수집 데이터 전처리 및 취합 시각화 데이터 수집의 단계는 온라인 커뮤니티에 올라오는 국민프로듀서들의 의견을 수집을 했는데, 모든 커뮤니티 데이터를 가저오는 것은 시간적으로 너무 많이 소요되기에 커뮤니티 사이트들 중 가장 활발했던 d모 사이트를 대상으로 크롤링을 진행하였습니다. 다음으로 데이터 전처리 및 취합은 수집한 데이터를 여러 기준들을 통해 나누는 전처리 과정인데, 이를테면 일간별로 수집한 데이터를 1주 간격으로 취합하고, 취합한 데이터들을 기준으로 각 연습생들의 언급량을 추출하는 과정입니다. 마지막으로 전처리를 끝마친 데이터를 이해하기 쉽도록 시각화를 진행 하였습니다. 이제 단계별로 차근차근 알아보겠습니다. "],
["intro.html", "Chapter 2 데이터 수집 2.1 code", " Chapter 2 데이터 수집 데이터 분석을 위해 가장 먼저 해야 할 일은 분석의 대상이 될 데이터를 확보 하는 일 입니다. 이전 글에서 말했다시피 이번 파트에서 진행할 것은 데이터 크롤링에 관한 전반적인 것들 입니다. 프로듀스X101 관련 모든 커뮤니티의 글들을 수집하는것은 물리적으로 불가능에 가깝기 때문에 해당방송과 관련된 커뮤니티중 가장 규모가 큰 한곳을 선정하여 데이터 수집을 진행하였습니다. Figure 2.1: 디시인사이드 프로듀스x101 갤러리 해당 커뮤니티는 방송이 인기를 끌었던 시점에 하루에 대략 4만건의 게시물이 올라왔었는데요. 한참 방송이 진행중이던 06/01 부터 마지막 생방송날인 07/19 까지의 데이터를 수집하였었습니다. 웹 데이터 수집을 위한 여러 방법이 있지만 R 에서 웹페이지를 크롤링할때 보편적으로 사용하는 rvest 패키지를 활용하여 크롤링을 진행하였습니다. 아래는 크롤링 코드 입니다. 2.1 code # 저장 폴더 설정 setwd(&#39;C:/Users/JDW/Desktop/PROJECT/PRODUCEX/CRAWLING DATA&#39;) library(rvest) library(dplyr) library(lubridate) library(stringr) 먼저 크롤링한 데이터를 저장할 폴더와 필요한 패키지들을 호출 합니다. basic_url &lt;- &#39;https://gall.dcinside.com/board/lists/?id=producex&amp;page=&#39; # 주소설정 ---- urls &lt;- NULL for(x in 0:599){ urls[x + 1] &lt;- paste0(basic_url, x + 1) } 웹 데이터를 크롤링해 오는데엔 두가지 방식이 있습니다. 바로 GET 방식과 POST방식인데요. GET 방식은 웹페이지의 URL주소를 기반으로 데이터를 가저오는 형식입니다. 즉, 특정한 웹페이지의 주소를 서버측에 요청하면 서버는 요청에 대한 응답으로 웹페이지 URL의 정보를 클라이언트(사용자)에게 보내주는 것입니다. 우리가 인터넷을 사용할때 주소창에 URL주소를 입력하고 엔터를 누를때 웹페이지가 변경되는 것을 생각하시면 됩니다. R에서는 이러한 동작의 수행을 위해서 rvest 패키지를 보편적으로 사용합니다. 웹페이지 주소를 기반으로 정보를 가져오기 때문에 정보를 가져올 웹페이지의 주소를 for문을 통해서 만들어 줍니다. 해당 코드를 돌리면 다음과 같은 결과값이 나옵니다. ## [1] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=1&quot; ## [2] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=2&quot; ## [3] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=3&quot; ## [4] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=4&quot; ## [5] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=5&quot; ## [6] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=6&quot; ## [1] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=595&quot; ## [2] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=596&quot; ## [3] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=597&quot; ## [4] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=598&quot; ## [5] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=599&quot; ## [6] &quot;https://gall.dcinside.com/board/lists/?id=producex&amp;page=600&quot; for문을 통해 1페이지부터 600페이지까지 총 600개의 웹페이지 주소가 생성되었네요. 만들어진 주소들을 기반으로 크롤링을 진행합니다. 먼저 크롤링 정보를 담을 빈객체들을 생성합니다. # 크롤링 ---- html1 &lt;- NULL p_title &lt;- NULL p_writer &lt;- NULL p_writer_id &lt;- NULL p_time &lt;- NULL p_count &lt;- NULL p_recommend &lt;- NULL dc &lt;- NULL 본격적으로 for문을 통해 데이터를 페이지순으로 순차적으로 가져옵니다. for(url in urls){ html1 &lt;- read_html(url) p_title &lt;- c(p_title, html1 %&gt;% html_nodes(&#39;.ub-content.us-post&#39;) %&gt;% html_nodes(&#39;.gall_tit.ub-word&#39;) %&gt;% html_node(&#39;a&#39;) %&gt;% html_text()) p_writer &lt;- c(p_writer, html1 %&gt;% html_nodes(&#39;.ub-content.us-post&#39;) %&gt;% html_nodes(&#39;.gall_writer.ub-writer&#39;) %&gt;% html_attr(&#39;data-nick&#39;)) p_writer_id &lt;- c(p_writer_id, html1 %&gt;% html_nodes(&#39;.ub-content.us-post&#39;) %&gt;% html_nodes(&#39;.gall_writer.ub-writer&#39;) %&gt;% html_attr(&#39;data-uid&#39;)) p_time &lt;- c(p_time, html1 %&gt;% html_nodes(&#39;.ub-content.us-post&#39;) %&gt;% html_nodes(&#39;.gall_date&#39;) %&gt;% html_attr(&#39;title&#39;)) p_count &lt;- c(p_count, html1 %&gt;% html_nodes(&#39;.ub-content.us-post&#39;) %&gt;% html_nodes(&#39;.gall_count&#39;) %&gt;% html_text()) p_recommend &lt;- c(p_recommend, html1 %&gt;% html_nodes(&#39;.ub-content.us-post&#39;) %&gt;% html_nodes(&#39;.gall_recommend&#39;) %&gt;% html_text()) } 해당 for문을 돌릴시 R은 맨 처음 만들었던 주소들 즉, urls 객체에 담긴 주소값을 받아서 서버에게 순차적으로 주소에 대한 정보를 요청하게 됩니다. 그리고 for문 내부에 있는 코드를 수행하는데요, 각각의 객체에 해당하는 값들을(p_title, p_time 등등..) 파싱합니다. 모든 코드를 수행하면 for문은 다음 주소를 요청하여 처음 코드부터 다시 파싱을 진행하게 됩니다. # 데이터 프레임화 dc &lt;- data.frame(p_title, p_writer, p_writer_id, p_time, p_count, p_recommend) # 날짜조정 ---- dc$p_time &lt;- ymd_hms(dc$p_time) dc &lt;- dc[day(dc$p_time) == day(today()-1), ] for문을 통해 수집한 각각의 객체 데이터들을 ‘dc’ 라는 이름의 데이터프레임에 담아줍니다. 시간을 다루는 lubridate 패키지의 ymd_hms()함수를 이용하여 글 작성 시간을 의미하는 p_time 데이터를 시간 데이터로 변경해 줍니다. 저는 이 코드를 taskscheduleR 패키지를 활용하여 매일 자정무렵 자동으로 돌아가게끔 설정 하였었는데요, 데이터를 매일 수집하는 것이다 보니 오직 전날에 작성된 글들만을 저장하기 위해서 dc &lt;- dc[day(dc$p_time) == day(today()-1), ] 필터링 코드를 만들었습니다. 코드를 짧막하게 설명하자면 글 작성 시간을 의미하는 dc$p_time 이 수집하는 날 바로 하루 전 today() - 1 의 값과 동일한 값만을 dc객체로 새로이 담아낸 것입니다. V1 p_title p_writer p_writer_id p_time p_count p_recommend 42448 ??? ㅇㅇ 2019-06-01 00:00:02 5 0 42449 시즌4 비센이 잇긴 하냐 ㅇㅇ 2019-06-01 00:00:01 11 1 42450 아아아아앙 민희야어아아아앙 ㅇㅇ 2019-06-01 00:00:01 4 0 42451 아악 유리옵ㅠㅠㅠㅠㅠㅠ 유리는유리다 yuri1228 2019-06-01 00:00:00 40 0 42452 김민규 그와중에 또 핑크입은거 실화냐 ㅇㅇ adkljf 2019-06-01 00:00:00 36 0 42453 진짜 한남들 자적자 어뜩하냐 ㅇㅇ 2019-06-01 00:00:00 27 1 # 파일저장 ---- write.csv(dc, paste0(ymd(today()-1), &quot;.csv&quot;)) 마지막으로 필터링까지 완료된 데이터를 csv파일로 저장합니다. "],
["데이터-전처리.html", "Chapter 3 데이터 전처리 3.1 데이터 전처리 3.2 변수열 생성", " Chapter 3 데이터 전처리 3.1 데이터 전처리 이번 파트는 데이터 전처리 과정을 다룹니다. 크롤링한 데이터들은 사용하기 위해서는 적합한 형태로 변환을 해야 하는 과정이 필요합니다. 크롤링한 데이터를 그대로 사용하는 경우는 극히 드물다고 할 수 있습니다. 데이터 분석의 과정을 요리에 비유하는 분들이 많이 계시는데요. 크롤링과 같은 수단을 통해서 데이터를 확보하는 과정을 재료 확보라고 생각을 해 보자면 데이터를 정제하고 전처리하는 과정을 요리에선 재료를 다듬고 손질하는 과정으로 생각할 수 있습니다. 마트에서 사온 채소나 고기등을 손질하지 않고서 그대로 요리할순 없듯이 우리가 확보한 데이터를 전처리 하지 않고서 분석을 하는것은 극히 드물다고 할 수 있습니다. 파일들을 R로 불러들이겠습니다. # ---- 패키지 호출 ---- library(dplyr) library(ggplot2) library(lubridate) library(stringr) library(gtools) # ---- for문을 활용한 데이터 불러오기 및 취합 ---- dir &lt;- &#39;C:/Users/JDW/Desktop/PROJECT/PRODUCEX/CRAWLING DATA/data&#39; filelist &lt;- list.files(dir) filelist &lt;- mixedsort(filelist, decreasing = T) pxdata &lt;- data.frame() temp &lt;- NULL for(file in filelist){ if(str_sub(file, -3, -1) == &#39;csv&#39;){ temp &lt;- fread(paste(dir, file, sep = &#39;/&#39;), header = T, stringsAsFactors = F) temp &lt;- temp %&gt;% select(p_title, p_time) pxdata &lt;- rbind(pxdata, temp) rm(temp) }else{ next } } 코드의 해석은 다음과 같습니다. 먼저 데이터가 들어있는 폴더의 경로를 dir 객체에 담아 둡니다. 다음으로 r의 내장함수인 list.files() 함수를 이용하여 폴더에 들어있는 파일들을 파악하고, 이를 filelist라는 이름의 객체에 담았습니다. gtools::mixedsort() 함수는 r의 기본 sort()함수가 처리하지 못하는 문자열형태의 숫자를 할 때에 사용되는 함수입니다. list.files(dir)로 불러들인 filelist 가 문자열이기 때문에 인간이 상식적으로 생각하는 형태로 정렬을 하지 못합니다. 간단한 예를 들어 설명해 보겠습니다. 3.1.1 mixedsort() 함수 t1 &lt;- c(1:20) %&gt;% as.character() t2 &lt;- c(1:20) %&gt;% as.numeric() str(t1) ## chr [1:20] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; ... str(t2) ## num [1:20] 1 2 3 4 5 6 7 8 9 10 ... t1객체와 t2 객체는 각각 1~20 까지의 데이터가 들어가 있는데요. t1은 문자열로 이루어져 있고, t2는 숫자로 구성되어 있습니다. 한번 내장함수인 sort() 사용하여 동일한 결과가 나오는지 비교해 보겠습니다. sort(t1) ## [1] &quot;1&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;2&quot; &quot;20&quot; &quot;3&quot; &quot;4&quot; ## [16] &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; sort(t2) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 숫자형으로 되어있는 t2 객체는 정상적으로 정렬이 된 반면 문자형의 t1 은 1 10 11 … 형태처럼 정렬이 안되었네요. 이번에는 gtools::mixedsort() 함수를 사용해 보겠습니다. library(gtools) mixedsort(t1) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## [16] &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; mixedsort(t2) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 숫자형인 t2는 이전처럼 정렬이 되었군요. 문자형인 t1은 이번엔 제대로 정렬된 것을 확인할 수 있습니다. 숫자로 되어진 문자열의 정렬을 위해서 종종 mixedsort() 함수를 사용하곤 합니다. 3.2 변수열 생성 데이터 분석에 사용할 변수는 게시물 제목에 해당하는 p_title 변수와, 게시물 작성 시간에 해당하는 p_time 입니다. 연습생들의 언급량이 에피소드별로 (1주간격으로) 어떻게-얼마나 변화했는지를 파악하는것이 본 데이터분석의 궁극적인 목적이기 때문에 분석에 대상이 되는 텍스트와 시간 변수를 제외한 나머지 데이터들은 제외해 주겠습니다. pxdata &lt;- pxdata %&gt;% select(p_title, p_time) 그런다음 글 작성 시간을 의미하는 p_time 열을 사용하여 ‘년월일’, ‘요일’ 변수열을 생성해 주겠습니다. pxdata &lt;- pxdata %&gt;% mutate(p_ymd = as.Date(p_time), p_wday = lubridate::wday(as.Date(p_time), label = T), ) 에피소드별로 연습생들의 언급량이 어떻게 변화하였는지를 파악하기 위해서는 데이터를 에피소드별로 구분할 필요가 있습니다. 방송은 매주 금요일 밤 11시에 시작하여 그 다음날 토요일 오전 12시30분즈음 끝마쳤는데요. 그렇기에 토요일을 시작으로 금요일까지의 데이터를 에피소드의 영향력 구간이라고 설정하고, 데이터를 구분지었습니다. 05월31일은 프로듀스X101 5화가 방영된 날이였기 때문에 수집을 시작한 06월01일을 에피소드 5화(p_weeknum = 5)로 설정하고 데이터를 순차적으로 구분지었네요. pxdata$p_weeknum &lt;- week(as.Date(pxdata$p_time) - 5) - 17 p_title p_time p_ymd p_wday p_weeknum 비율갑 연습생이 있다? 2019-06-01 23:59:50 2019-06-01 토 5 디모데 떨어진거 너무 아깝다 2019-06-01 23:59:48 2019-06-01 토 5 픽 남는데 추천해 줘 2019-06-01 23:59:43 2019-06-01 토 5 투표할때 2019-06-01 23:59:42 2019-06-01 토 5 남자 새끼들이 병신처럼 쳐울거나 2019-06-01 23:59:31 2019-06-01 토 5 근데 김요한은 운 하나는 존나 좋은거같아서 부럽긔 2019-06-01 23:59:25 2019-06-01 토 5 김성현 꼰대기 1도 없고 하찮다&lt;U+270B&gt; 2019-06-01 23:59:05 2019-06-01 토 5 캔디 롤 찍는 거냐?? 외로워도 슬퍼도 나는 안울어 2019-06-01 23:58:54 2019-06-01 토 5 남도현 움짤 프추좀 눌러주시긔 2019-06-01 23:58:52 2019-06-01 토 5 어제 상위권애들 왤케 거만했냐? 좆같게 2019-06-01 23:58:51 2019-06-01 토 5 변수열이 생성된 데이터의 모습 전처리 과정을 통하여 필요없는 변수열들은 제거하고, 분석에 필요한 새로운 변수열 (p_ymd, p_wday, p_weeknum)을 생성하였습니다. 전처리한 데이터를 기반으로 다음장에서 본격적인 분석 및 시각화를 진행해 보도록 하겠습니다. "],
["eda.html", "Chapter 4 EDA 4.1 dataset 4.2 회차별 게시글 수 비교 4.3 요일별 게시글 수 비교 4.4 일자별 게시글 수 비교", " Chapter 4 EDA 본격적인 분석에 앞서 이전 파트에서 전처리한 데이터들을 기반으로 간단한 EDA를 진행해 볼까 합니다. 시각화적 EDA를 통해 지금 가지고있는 데이터의 특성을 파악하고, 특별히 주목해야할 정보가 있는지를 한번 살펴 보겠습니다. 먼저 앞장에서 다루었던 전처리한 데이터 입니다. 4.1 dataset DT::datatable(head(pxdata, 10)) 4.2 회차별 게시글 수 비교 pxdata %&gt;% group_by(p_weeknum) %&gt;% count() ## # A tibble: 8 x 2 ## # Groups: p_weeknum [8] ## p_weeknum n ## &lt;dbl&gt; &lt;int&gt; ## 1 4 42453 ## 2 5 158394 ## 3 6 193814 ## 4 7 219135 ## 5 8 201851 ## 6 9 196579 ## 7 10 259003 ## 8 11 227058 ## `summarise()` ungrouping output (override with `.groups` argument) 그래프의 x축은 방송 회차 기간을 의미하고, y축은 작성글 수를 의미합니다. 방송이 진행될수록 국민프로듀서(시청자) 들의 작성글 수는 증가하는 면모를 보이네요. 4.3 요일별 게시글 수 비교 pxdata %&gt;% group_by(p_wday) %&gt;% count() ## # A tibble: 7 x 2 ## # Groups: p_wday [7] ## p_wday n ## &lt;ord&gt; &lt;int&gt; ## 1 일 197025 ## 2 월 185848 ## 3 화 162261 ## 4 수 160101 ## 5 목 169444 ## 6 금 265090 ## 7 토 358518 ## `summarise()` ungrouping output (override with `.groups` argument) 방송이 방영되던 금요일과, 방송이 끝난 당일인 토요일에 작성된 글 수가 많은 것을 확인할 수 있습니다. 4.4 일자별 게시글 수 비교 pxdata %&gt;% group_by(p_ymd) %&gt;% count() %&gt;% mutate(p_wday = lubridate::wday(p_ymd, label = T), p_md = str_sub(p_ymd, 6, 10)) %&gt;% ggplot(aes(x = as.factor(p_md), y = n)) + geom_col(aes(fill = p_wday)) + theme(axis.text.x = element_text(angle = 45, size = 7)) + labs(x = &#39;ymd&#39;, y = &#39;post&#39;, fill = &quot;&quot;) pxdata %&gt;% group_by(p_ymd) %&gt;% count() %&gt;% mutate(p_wday = lubridate::wday(p_ymd, label = T), p_md = str_sub(p_ymd, 6, 10)) %&gt;% ggplot(aes(x = reorder(as.factor(p_md), -n), y = n)) + geom_col(aes(fill = p_wday)) + theme(axis.text.x = element_text(angle = 45, size = 7)) + labs(x = &#39;ymd&#39;, y = &#39;post&#39;, fill = &quot;&quot;) pxdata %&gt;% group_by(p_ymd) %&gt;% count() %&gt;% mutate(p_wday = lubridate::wday(p_ymd, label = T), p_md = str_sub(p_ymd, 6, 10)) %&gt;% ggplot(aes(x = as.factor(p_md), y = n)) + geom_col() + geom_text(aes(label = n), vjust = -1, size = 2) + theme(axis.text.x = element_text(size = 0)) + facet_wrap(~ p_wday) + labs(x = &#39;ymd&#39;, y = &#39;post&#39;) 일자별로 나누어 보았을 경우에도 금요일과 토요일에 사람들의 프로그램에 대한 관심도가 높은것을 확인할 수 있습니다. EDA는 여기서 마무리 짓고 다음장에서 본격적으로 어느 연습생의 인기가 많았는지를 분석을 통해 도출해 보도록 하겠습니다. "],
["시각화.html", "Chapter 5 시각화 5.1 데이터 시각화 5.2 범위 설정 5.3 언급량 집계 5.4 시각화", " Chapter 5 시각화 이제 분석의 마지막 작업으로 지금까지 전처리한 데이터를 기반으로 어느 연습생의 언급량이 높았었는지를 직접 확인해 볼 시간입니다. 데이터 크롤링부터 이전 시간에 진행한 EDA까지 다소 짧은 분량은 아니었는데요. 이번 시각화를 통해서 분석을 마무리를 지어볼까 합니다. 5.1 데이터 시각화 지금까지 수집한 데이터는 실제로 방송을 시청하는 시청자들이 게시판에 작성했던 게시물들의 제목들(p_title) 입니다. 즉, 방송을 보고 투표를 할 확률이 높은 시청자들의 의견이 담긴 텍스트인데요. 처음 분석을 기획하였을 당시 저는 먼저 한가지 가정을 했었습니다. 그건 바로 언급량이 높으면 높을수록 그만큼 인기도 많을 것 이였는데요. 사람들 입에서 회자가 많이 된다는 것은 그만큼 관심과 인기가 있다는 의미이고, 투표량도 그것과 상관이 있을것이다라고 가정하였습니다. 5.2 범위 설정 프로듀스X101은 101명의 연습생들이 경쟁을 펼치고, 시청자들의 투표를 통해서 최종 11명의 아이돌 멤버를 선발한다는 컨셉의 방송인데요. 마지막 방송날에 인기가 높았던 20명의 연습생들이 생방송 무대에서 경합을 펼쳤습니다. 마지막 방송까지 살아남은 최종 20명을 대상으로 분석을 진행해 보도록 하겠습니다. 미리 마지막 생방송에 진출한 스무명의 연습생 이름을 final_boylist.txt 파일에 저장해 두었었고, 이를 불러오겠습니다. boylist &lt;- readLines(&#39;./final_boylist.txt&#39;) boylist ## [1] &quot;김요한&quot; &quot;김우석&quot; &quot;이진혁&quot; &quot;한승우&quot; &quot;김민규&quot; &quot;조승연&quot; &quot;남도현&quot; &quot;송형준&quot; ## [9] &quot;이은상&quot; &quot;금동현&quot; &quot;차준호&quot; &quot;손동표&quot; &quot;황윤성&quot; &quot;강민희&quot; &quot;구정모&quot; &quot;이한결&quot; ## [17] &quot;송유빈&quot; &quot;함원진&quot; &quot;토니&quot; &quot;이세진&quot; 언급량은 연습생별로 연습생이 언급된 게시물들의 갯수가 몇개인지를 세어보는 방식으로 집계할 것입니다. 그런데 게시물들의 특성상 연습생의 fullname을 적는 경우보다 name만 적는 경우가 많이 있었기에 먼저 호출한 boylist에서 이름만 추출하겠습니다. boylist2 &lt;- str_sub(boylist, -2, -1) # 뒤에서 두번째 글자부터 뒤에서 첫번째 글자까지만을 저장 boylist2 ## [1] &quot;요한&quot; &quot;우석&quot; &quot;진혁&quot; &quot;승우&quot; &quot;민규&quot; &quot;승연&quot; &quot;도현&quot; &quot;형준&quot; &quot;은상&quot; &quot;동현&quot; ## [11] &quot;준호&quot; &quot;동표&quot; &quot;윤성&quot; &quot;민희&quot; &quot;정모&quot; &quot;한결&quot; &quot;유빈&quot; &quot;원진&quot; &quot;토니&quot; &quot;세진&quot; 그런다음 for문을 통해 연습생별로 언급량을 집계해 보겠습니다. 5.3 언급량 집계 boys &lt;- NULL for(i in 1:length(boylist2)){ boys[i] &lt;- nrow(pxdata[grep(boylist2[i], pxdata$p_title), ]) #연습생별 언급게시글 수를 boys 리스트에 저장 } boys_comment &lt;- data.frame(cbind(boylist, boys), stringsAsFactors = F) # 연습생 - 연습생 언급게시글 수 데이터프레임화 boys_comment$boys &lt;- as.numeric(boys_comment$boys) # 숫자형으로 변환 str(boys_comment) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ boylist: chr &quot;김요한&quot; &quot;김우석&quot; &quot;이진혁&quot; &quot;한승우&quot; ... ## $ boys : num 36346 67610 26061 26593 47132 ... 이를 시각화하여 바로 알아보도록 하겠습니다. 5.4 시각화 5.4.1 Barchart boys_comment %&gt;% ggplot(aes(x = boylist, y = boys)) + geom_col() + geom_text(aes(label = boys), vjust = -.5, size = 2) + theme(axis.text.x = element_text(angle = 45)) boys_comment %&gt;% ggplot(aes(x = reorder(boylist, -boys), y = boys)) + geom_col() + geom_text(aes(label = boys), vjust = -.5, size = 2) + theme(axis.text.x = element_text(angle = 45)) 시각화를 진행한 결과 김우석, 조승연, 김민규, 구정모, 이은상, 김요한, 남도현, 송형준, 강민희, 한승우 , 이진혁 순으로 언급량이 많은것을 확인 할 수 있었습니다. 하지만 이것은 전체기간을(06/01 ~ 07/19) 대상으로 집계한 언급량이기 때문에 연습생들의 시간에 따른 언급량의 변화는 찾아 볼 수가 없네요. 방송이 진행되면서 멋진 무대를 소화해 낸 연습생은 순위가 올라간 경우도 있었고, 반대로 다른 여러 요인들로 인해 인기가 하락한 경우도 종종 볼 수 있었습니다. 그렇다면 이번에는 에피소드별로 연습생들의 언급량이 어떻게 변화했는지를 확인해 보도록 하겠습니다. 5.4.2 Barchart(Split by episode) boyname &lt;- NULL temp &lt;- NULL total &lt;- NULL for(i in 1:7){ assign(paste0(&#39;episode_&#39;, i + 4), data.frame()) # 빈 객체 생성 assign(paste0(&#39;episode_&#39;, i + 4), pxdata %&gt;% filter(p_weeknum == i + 4)) # 회차별로 데이터 나누기 boyname &lt;- NULL temp &lt;- NULL for(x in 1:length(boylist2)){ boyname[x] &lt;- nrow(get(paste0(&#39;episode_&#39;, i + 4))[grep(boylist2[x], get(paste0(&#39;episode_&#39;, i + 4))$p_title), ]) } temp &lt;- as.data.frame(cbind(boylist2, boyname)) temp$weeknum &lt;- i + 4 total &lt;- rbind(total, temp) } assign() 함수는 변수를 만들때 사용하는 함수입니다. for문 내부에서는 할당연산자인 = 혹은 &lt;- 를 사용할수가 없는데요, for문 내에서 특정한 변수를 생성해야 할 경우가 발생할 수 있습니다. 그럴경우 assign('변수명', '변수')의 형태를 통해 변수를 만드실 수 있습니다. 코드를 잠깐 해석하자면 assign() 함수를 통해 episode_5 ~ episode_11 까지의 변수를 생성하고, 각각의 변수에다가 변수 이름에 맞는 데이터들을 분할하여 넣어주었습니다. 그런다음 다시 내부의 for문을 통해 각 에피소드별 연습생들의 언급량을 구하였고(boyname 변수), 이를 연습생의 이름칼럼과 합친 temp라는 이름의 데이터 프레임으로 생성해 주었습니다. 그리고 최종적으로 rbind() 함수를 통해 total 이란 변수에 접합을 해주었습니다. 말로 설명하자니 꽤나 추상적이군요 이번에는 total 변수 직접 확인해 보겠습니다. DT::datatable(total) 이렇게 에피소드별, 연습생별로 나누어진 데이터를 Barchart를 통해 시각화 해보겠습니다. colnames(total) &lt;- c(&#39;boyname&#39;, &#39;mention&#39;, &#39;weeknum&#39;) total$mention &lt;- as.numeric(as.character(total$mention)) total %&gt;% ggplot(aes(x = as.factor(weeknum), y = mention)) + geom_col() + geom_text(aes(label = mention), vjust = -1, size = 2) + facet_wrap( ~ boyname) + labs(x = &#39;episode&#39;, y = &#39;mention&#39;) 확실히 시간에 따른(에피소드) 연습생의 언급량이 어떻게 변화하였는지가 파악이 되는군요. 그렇지만 언급량 순으로 정렬이 안돼있어서 직관적으로 파악하기가 어렵습니다. 한번 11주차 방송 기간을 기준으로 연습생의 언급도를 재정렬해 보겠습니다. 5.4.3 Barchart(Split by episode and reordered) pxlevel &lt;- total %&gt;% filter(weeknum == 11) %&gt;% arrange(desc(mention)) %&gt;% select(boyname) %&gt;% as.list() pxlevel &lt;- melt(pxlevel) pxlevel &lt;- pxlevel$value pxlevel &lt;- as.character(pxlevel) total_ordered &lt;- arrange(transform(total, boyname = factor(boyname, levels = pxlevel)), boyname) total_ordered %&gt;% ggplot(aes(x = as.factor(weeknum), y = mention, group = boyname)) + geom_col() + facet_wrap( ~ boyname) + geom_text(aes(label = mention), vjust = -1, size = 2) + labs(title = &#39;[PRODUCEX101] 회차별 연습생 언급량 변화 추이&#39;, subtitle = &#39;&quot;ep.11&quot; 기준 정렬&#39;, x = &#39;episode&#39;, y = &#39;mention&#39;) 11회차를 기준으로는 조승연, 구정모, 김우석, 김민규, 남도현, 한승우, 이은상, 강민희, 이진혁, 김요한, 송형준 순으로 언급된 것을 볼 수 있습니다. 실제로 데뷔한 연습생들과 비교해 보니 전체 11명중 8명이 속하는군요 (물론 잘못된 투표였다는 발표가 있었지만요…) 하지만 온라인 커뮤니티에서 실제 사람들이 누구를 많이 얘기하고, 누구를 상대적으로 적게 얘기하는지를 객관적으로 측정할 수 있는 분석이였습니다. 이제 정말 마지막으로 Bump chart를 제작하여 연습생들의 언급도가 어떻게 변화하였는지를 확인해 보도록 하겠습니다. 5.4.4 Bump chart Figure 5.1: Bump chart Bump chart는 순위가 있는 값들을 시각화하는데 사용이 되는 차트인데요. 위와 같이 순위의 변화를 관찰할때 유용하게 사용되는 차트입니다. ```r library(plotly) # data setting total_ranking &lt;- total %&gt;% group_by(weeknum) %&gt;% arrange(-mention) %&gt;% mutate(ranking = row_number()) %&gt;% as.data.frame() # theme my_theme &lt;- function() { # Colors color.background = &quot;white&quot; color.text = &quot;#22211d&quot; # Begin construction of chart theme_bw(base_size=15) + # Format background colors theme(panel.background = element_rect(fill=color.background, color=color.background)) + theme(plot.background = element_rect(fill=color.background, color=color.background)) + theme(panel.border = element_rect(color=color.background)) + theme(strip.background = element_rect(fill=color.background, color=color.background)) + # Format the grid theme(panel.grid.major.y = element_blank()) + theme(panel.grid.minor.y = element_blank()) + theme(axis.ticks = element_blank()) + # Format the legend # theme(legend.position = &quot;none&quot;) + # Format title and axis labels theme(plot.title = element_text(color=color.text, size=20, face = &quot;bold&quot;)) + theme(axis.title.x = element_text(size=14, color=&quot;black&quot;, face = &quot;bold&quot;)) + theme(axis.title.y = element_text(size=14, color=&quot;black&quot;, face = &quot;bold&quot;, vjust=1.25)) + theme(axis.text.x = element_text(size=10, vjust=0.5, hjust=0.5, color = color.text)) + theme(axis.text.y = element_text(size=10, color = color.text)) + theme(strip.text = element_text(face = &quot;bold&quot;)) + # Plot margins theme(plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), &quot;cm&quot;)) } # Visualizing total_ranking %&gt;% ggplot(aes(x = weeknum, y = ranking, group = boyname)) + geom_line(aes(color = boyname, alpha = 1), size = 2) + geom_point(aes(color = boyname, alpha = 1), size = 4) + geom_point(color =&#39;#FFFFFF&#39;, size = 1) + scale_x_continuous(breaks = 5: 11, minor_breaks = 5:11, expand = c(.05, .05)) + scale_y_reverse(breaks = 1:20) + geom_text(data = total_ranking %&gt;% filter(weeknum == 5), aes(label = boyname, x = 4.5), hjust = 2, fontface = &#39;bold&#39;, color = &#39;#888888&#39;, size = 5) + geom_text(data = total_ranking %&gt;% filter(weeknum == 11), aes(label = boyname, x = 11.5), hjust = 2, fontface = &#39;bold&#39;, color = &#39;#888888&#39;, size = 5) + theme(legend.position = &#39;none&#39;) + labs(title = &#39;[PRODUCEX101 GALLARY] WEEKLY BUMPCHART&#39;, subtitle = &#39;separated by episode&#39;, x = &#39;episode&#39;, y = &#39;ranking&#39;, alpha = &quot;&quot;, color = &quot;&quot;) + my_theme() -&gt; p ggplotly(p) ``` 이름을 더블클릭하면 추세를 명확하게 확인할 수 있습니다. Bumpchart는 특별히 plotly를 활용하여 반응형 차트를 제작해 보았습니다. 마우스액션을 통해 좀더 세분화되게 차트를 탐색해 볼 수 있습니다. 지금까지 데이터를 수집부터, 시각화 분석까지의 전반적인 과정들을 한번 훝어보았습니다. 예전에 취미삼아 혼자서만 분석하고 묵혀만 두었던 프로젝트였는데 블로그를 구축하고, 많은 분들께 공유할 수 있게 되어서 감회가 새롭네요. R 유저로서 blogdown 패키지로 한번 블로그를 만들어 보고 싶었는데 올해가 넘어가기 전에 그 일을 끝마칠수 있어서 개인적으로 뿌듯합니다. 처음 작성한 글이라 미흡한 글이긴 하지만 도움이나 흥미가 있었기를 바라며 이만 마무리 짓도록 하겠습니다. 지금까지 긴글 읽어주셔서 감사합니다. "]
]
